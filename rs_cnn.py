# -*- coding: utf-8 -*-
"""rs_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aK8JENa2NM0E-MUF42mZjkedMoPJsglF
"""

!pip install tensorflow

import tensorflow as tf
from tensorflow import keras
from keras import datasets, layers, models, callbacks

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer, accuracy_score
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import loguniform, uniform

import time
import warnings

warnings.filterwarnings('ignore')
# Make scorer accuracy
score_acc = make_scorer(accuracy_score)

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Set hyperparameter space
hyperspace_cnn = {
    'conv1_act': np.linspace(0, 8),
    'conv2_act': np.linspace(0, 8),
    'conv3_act': np.linspace(0, 8),
    'conv1_ker': np.linspace(2, 10),
    'conv2_ker': np.linspace(2, 10),
    'conv3_ker': np.linspace(2, 10),
    'conv1_filter': np.linspace(20, 32),
    'conv2_filter': np.linspace(20, 32),
    'conv3_filter': np.linspace(50, 100),
    'pool1_ker': np.linspace(2, 4),
    'pool2_ker': np.linspace(2, 4),
    'pool3_ker': np.linspace(2, 4),
    'dense1_neuron': np.linspace(32, 64),
    'dense2_neuron': np.linspace(32, 64),
    'dense1_act': np.linspace(0, 7),
    'dense2_act': np.linspace(0, 7),
    'dropout_rate':np.linspace(0, 0.5),
    'learning_rate':uniform(0.0005, 0.002),
    'beta_1':np.linspace(0.9,1),
    'beta_2':np.linspace(0.95,1),
 }

def tune_cnn_rs(param_space, iter=2, random_state=545):
    '''
    Using Random Search to tune the parameters of XGBoost. 
    Task: Binary Search
    Output: logloss, best_params
    '''
    def create_cnn(conv1_act,conv2_act,conv3_act,conv1_ker,conv2_ker,conv3_ker,conv1_filter,conv2_filter,conv3_filter,
                pool1_ker,pool2_ker,pool3_ker,dense1_act,dense2_act,dense1_neuron,dense2_neuron,dropout_rate,learning_rate,beta_1,beta_2):
        activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu','elu', 'exponential']
    
        conv1_act = activationL[int(conv1_act)]
        conv2_act = activationL[int(conv2_act)]
        conv3_act = activationL[int(conv3_act)]
        dense1_act = activationL[int(dense1_act)]
        dense2_act = activationL[int(dense2_act)]

        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2)
    
        model = models.Sequential()
        model.add(layers.Conv2D(int(conv1_filter), int(conv1_ker), activation=conv1_act, padding="same", input_shape=(32, 32, 3)))
        model.add(layers.MaxPooling2D(int(pool1_ker)))
        model.add(layers.Conv2D(int(conv2_filter), int(conv2_ker), activation=conv2_act, padding="same"))
        model.add(layers.MaxPooling2D(int(pool2_ker)))
        model.add(layers.Conv2D(int(conv3_filter), int(conv3_ker), activation=conv3_act, padding="same"))
        model.add(layers.MaxPooling2D(int(pool3_ker)))
        model.add(layers.Flatten())
        model.add(layers.Dense(int(dense1_neuron), activation=dense1_act))
        model.add(layers.Dropout(dropout_rate))
        model.add(layers.Dense(int(dense2_neuron), activation=dense2_act))
        model.add(layers.Dense(10))
    
        model.compile(optimizer = opt,
                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    
        return model
    
    model_CV = KerasClassifier(build_fn=create_cnn, epochs=2, verbose=1)
    
    rs_model = RandomizedSearchCV(estimator = model_CV,
                               param_distributions = param_space,
                               n_iter = iter,
                               n_jobs = -1,
                               verbose = 3,
                               random_state = random_state,
                               cv = 2)
    
    rs_model.fit(train_images,train_labels)
    return rs_model.cv_results_['mean_test_score'], rs_model.best_params_

# Run Random Search 
start = time.time()
tuned_xgb = tune_cnn_rs(hyperspace_cnn)
end = time.time()
print("The execution time of BO is :", end-start)

best_param = tuned_xgb[1]
print(best_param)



conv1_ker = best_param['conv1_ker']
conv2_ker = best_param['conv2_ker']
conv3_ker = best_param['conv3_ker']
conv1_filter = best_param['conv1_filter']
conv2_filter = best_param['conv2_filter']
conv3_filter = best_param['conv3_filter']
pool1_ker = best_param['pool1_ker']
pool2_ker = best_param['pool2_ker']
pool3_ker = best_param['pool3_ker']
dense1_neuron = best_param['dense1_neuron']
dense2_neuron = best_param['dense2_neuron']
dropout_rate = best_param['dropout_rate']
learning_rate = best_param['learning_rate']
beta_1 = best_param['beta_1']
beta_2 = best_param['beta_2']


opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2)

                
activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu','elu', 'exponential']  
conv1_act = activationL[int(best_param['conv1_act'])]
conv2_act = activationL[int(best_param['conv2_act'])]
conv3_act = activationL[int(best_param['conv3_act'])]
dense1_act = activationL[int(best_param['dense1_act'])]
dense2_act = activationL[int(best_param['dense2_act'])]

model = models.Sequential()
model.add(layers.Conv2D(int(conv1_filter), int(conv1_ker), activation=conv1_act, padding="same", input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D(int(pool1_ker)))
model.add(layers.Conv2D(int(conv2_filter), int(conv2_ker), activation=conv2_act, padding="same"))
model.add(layers.MaxPooling2D(int(pool2_ker)))
model.add(layers.Conv2D(int(conv3_filter), int(conv3_ker), activation=conv3_act, padding="same"))
model.add(layers.MaxPooling2D(int(pool3_ker)))
model.add(layers.Flatten())
model.add(layers.Dense(int(dense1_neuron), activation=dense1_act))
model.add(layers.Dropout(dropout_rate))
model.add(layers.Dense(int(dense2_neuron), activation=dense2_act))
model.add(layers.Dense(10))

model.compile(optimizer = opt,
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


es = callbacks.EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=5)


history = model.fit(train_images, train_labels, epochs=10, validation_split = 0.1, callbacks=[es])

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')


plt.figure()
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print(test_acc)

bo_val_loss = []
for i,res in enumerate(cnn_bo.res):
    bo_val_loss.append(-1.0 * cnn_bo.res[i]['target'])
    

loss = np.array(bo_val_loss)
np.minimum.accumulate(loss)
plt.figure()
plt.plot(np.minimum.accumulate(loss))