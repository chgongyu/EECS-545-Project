#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Apr 16 15:32:01 2022

@author: zixiao
"""
from bayes_opt import BayesianOptimization
import tensorflow as tf
from tensorflow.keras import datasets, layers, models, callbacks
from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl
import pandas as pd
pd.set_option("display.max_columns", None)
import matplotlib.pyplot as plt


from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer, accuracy_score
from keras.wrappers.scikit_learn import KerasClassifier

import time


import warnings
warnings.filterwarnings('ignore')
# Make scorer accuracy
score_acc = make_scorer(accuracy_score)

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

def bo_tune_cnn(batch_size,conv1_act,conv2_act,conv3_act,conv1_ker,conv2_ker,conv3_ker,conv1_filter,conv2_filter,conv3_filter,
                pool1_ker,pool2_ker,pool3_ker,dense1_act,dense2_act,dense1_neuron,dense2_neuron,optimizer,dropout_rate,learning_rate):
    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',
                   'elu', 'exponential']
    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']
    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),
                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),
                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),
                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}
    
    conv1_act = activationL[round(conv1_act)]
    conv2_act = activationL[round(conv2_act)]
    conv3_act = activationL[round(conv3_act)]
    dense1_act = activationL[round(dense1_act)]
    dense2_act = activationL[round(dense2_act)]
    
    optimizer = optimizerD[optimizerL[round(optimizer)]]
    
    def cnn_model():
        model = models.Sequential()
        model.add(layers.Conv2D(int(conv1_filter), int(conv1_ker), activation=conv1_act, padding="same", input_shape=(32, 32, 3)))
        model.add(layers.MaxPooling2D(int(pool1_ker)))
        model.add(layers.Conv2D(int(conv2_filter), int(conv2_ker), activation=conv2_act, padding="same"))
        model.add(layers.MaxPooling2D(int(pool2_ker)))
        model.add(layers.Conv2D(int(conv3_filter), int(conv3_ker), activation=conv3_act, padding="same"))
        model.add(layers.MaxPooling2D(int(pool3_ker)))
        model.add(layers.Flatten())
        model.add(layers.Dense(int(dense1_neuron), activation=dense1_act))
        model.add(layers.Dropout(dropout_rate))
        model.add(layers.Dense(int(dense2_neuron), activation=dense2_act))
        model.add(layers.Dense(10))
        
        model.compile(optimizer = optimizer,
                      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      metrics=['accuracy'])
        return model
    
    
    
    es = callbacks.EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)
    
    cnn = KerasClassifier(build_fn = cnn_model, epochs = 13, batch_size = int(batch_size),
                         )
    
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)
    score = cross_val_score(cnn, train_images, train_labels, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()
    return score

# Set hyperparameter space
hyperspace_cnn = {
    'conv1_act': (0, 7),
    'conv2_act': (0, 7),
    'conv3_act': (0, 7),
    'conv1_ker': (2, 10),
    'conv2_ker': (2, 10),
    'conv3_ker': (2, 10),
    'conv1_filter': (20, 50),
    'conv2_filter': (50, 100),
    'conv3_filter': (50, 100),
    'pool1_ker': (2, 4),
    'pool2_ker': (2, 4),
    'pool3_ker': (2, 4),
    'dense1_neuron': (32, 100),
    'dense2_neuron': (32, 100),
    'dense1_act': (0, 7),
    'dense2_act': (0, 7),
    'dropout_rate':(0, 0.5),
    'optimizer':(1, 2),
    'learning_rate':(0.01, 1),
    'batch_size':(20, 100),
 }

# Run Bayesian Optimization
cnn_bo = BayesianOptimization(bo_tune_cnn, hyperspace_cnn, random_state=123)
cnn_bo.maximize(init_points=5, n_iter=15)